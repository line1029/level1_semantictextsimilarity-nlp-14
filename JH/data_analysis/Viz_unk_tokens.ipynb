{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNK tokenì„ í¬í•¨í•œ ë¬¸ì¥ì—ëŠ” ë¬´ì—‡ì´ ìˆëŠ”ê°€ í™•ì¸í•˜ëŠ” ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ê¸°ì´ˆ í•¨ìˆ˜ ì„¸íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk sentence to csv\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "dev_path = './data/dev.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "dev_data = pd.read_csv(dev_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small')\n",
    "\n",
    "# Function Settings\n",
    "def get_num_tokens(df):\n",
    "    sentence1_len, sentence2_len = [], []\n",
    "    sentence1_unk, sentence2_unk = [], []\n",
    "    \n",
    "    for i, item in df.iterrows():\n",
    "        sentence1 = tokenizer(item['sentence_1'])['input_ids']\n",
    "        sentence2 = tokenizer(item['sentence_2'])['input_ids']\n",
    "\n",
    "        sentence1_len.append(len(sentence1))\n",
    "        sentence2_len.append(len(sentence2))\n",
    "\n",
    "        sentence1_unk.append(sentence1.count(tokenizer.unk_token_id))\n",
    "        sentence2_unk.append(sentence2.count(tokenizer.unk_token_id))\n",
    "\n",
    "    return sentence1_len, sentence2_len, sentence1_unk, sentence2_unk\n",
    "    # return pd.DataFrame({'number of tokens':sentence1_len, 'label score':df.label.values.tolist()})\n",
    "\n",
    "\n",
    "# 1. ì „ì²´ dfì— ëŒ€í•´ score 5ë‹¨ê³„ë¡œ ë¶„ë¥˜, ì—´ ì¶”ê°€\n",
    "train_data_scored = train_data.copy(deep=True)\n",
    "score_integer = []\n",
    "\n",
    "for i, item in train_data_scored.iterrows():\n",
    "    label_value = int(item['label'])\n",
    "    if   label_value == 0:  col = 0\n",
    "    elif label_value < 2.0: col = 1\n",
    "    elif label_value < 3.0: col = 2\n",
    "    elif label_value < 4.0: col = 3\n",
    "    elif label_value < 5.0: col = 4\n",
    "    else:                   col = 5\n",
    "        \n",
    "    score_integer.append(col)\n",
    "train_data_scored['score_class'] = score_integer\n",
    "\n",
    "# 2. sentence ë³„ í† í° ê°œìˆ˜ ë„£ê¸°\n",
    "s1_len, s2_len, s1_unk, s2_unk = get_num_tokens(train_data_scored)\n",
    "\n",
    "train_data_scored['s1_num_tokens'] = s1_len\n",
    "train_data_scored['s2_num_tokens'] = s2_len\n",
    "train_data_scored['s1_num_unk'] = s1_unk\n",
    "train_data_scored['s2_num_unk'] = s2_unk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. unk token ë¬¸ì¥ í™•ì¸ í›„ CSV ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> í† í°ì„ í¬í•¨í•˜ê³  ìˆëŠ” ë¬¸ì¥ì˜ ì „ì²´ ê°œìˆ˜ëŠ” 453 ê°œ ì…ë‹ˆë‹¤.\n",
      "í•´ë‹¹ ë¬¸ì¥ë“¤ì„ csvíŒŒì¼ë¡œ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„: unk_setences_train.csv\n"
     ]
    }
   ],
   "source": [
    "# unk sentence to csv\n",
    "\n",
    "unk1_token_sentence = []\n",
    "unk2_token_sentence = []\n",
    "unk1_count, unk2_count = [], []\n",
    "\n",
    "source_list = sorted(train_data['source'].unique())\n",
    "\n",
    "for source_name in source_list:\n",
    "    g = train_data_scored.groupby(['source']).get_group(source_name)\n",
    "    u1_sentence = g[g['s1_num_unk'] >=1]['sentence_1'].values.tolist()\n",
    "    u2_sentence = g[g['s2_num_unk'] >=1]['sentence_2'].values.tolist()\n",
    "\n",
    "    unk1_token_sentence.extend(u1_sentence)\n",
    "    unk2_token_sentence.extend(u2_sentence)\n",
    "    unk1_count.extend(g[g['s1_num_unk'] >=1]['s1_num_unk'].values.tolist())\n",
    "    unk2_count.extend(g[g['s2_num_unk'] >=1]['s2_num_unk'].values.tolist())\n",
    "\n",
    "_t_num = len(unk1_token_sentence) + len(unk2_token_sentence)\n",
    "print(f\"<UNK> í† í°ì„ í¬í•¨í•˜ê³  ìˆëŠ” ë¬¸ì¥ì˜ ì „ì²´ ê°œìˆ˜ëŠ” {_t_num} ê°œ ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "max_len = max(len(unk1_token_sentence), len(unk2_token_sentence))\n",
    "unk1_token_sentence += [''] * (max_len - len(unk1_token_sentence))\n",
    "unk1_count += [''] * (max_len - len(unk1_count))\n",
    "unk2_token_sentence += [''] * (max_len - len(unk2_token_sentence))\n",
    "unk2_count += [''] * (max_len - len(unk2_count))\n",
    "\n",
    "unk_pd = pd.DataFrame({'unk1_sentences': unk1_token_sentence, \n",
    "                       'unk1_count': unk1_count,\n",
    "                       'unk2_sentences': unk2_token_sentence,\n",
    "                       'unk2_count': unk2_count,\n",
    "                       })\n",
    "unk_pd.to_csv('./unk_setences_train.csv')\n",
    "print(f\"í•´ë‹¹ ë¬¸ì¥ë“¤ì„ csvíŒŒì¼ë¡œ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„: unk_setences_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ì–´ë–¤ ë‹¨ì–´ê°€ UNKë¡œ ì¸ì‹ë˜ì—ˆëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk ë¡œ ì¸ì‹ëœ í† í°ë“¤ : [['ì˜¤ë§ˆì´ê°€ëœ¨ì§€ì ¸ìŠ¤í¬ë¡¸ì´ìŠ¤íŠ¸íœ'], ['ë´£ëŠ”ë°'], ['ë´£ëŠ”ë°'], ['í€¼ë¦¬í‹°ë¥¼'], ['ë«ëŠ”ë°'], ['ì¤¸ìŸ'], ['ìŠ¤íƒ€ë€ì˜'], ['ê°–ì·ƒë‹¤'], ['ìŠ¤íƒ€ë€ì´ë‚˜'], ['ë°”ë¥´ë€ì˜'], ['ëì—‡ì§€ë§Œ'], ['ì•ˆë´£ë‹¤ã…¡ã…¡'], ['ì½±'], ['í­ê·„ì´'], ['ë©ì§„'], ['ë…ë…ë…'], ['ë´£ìŠµë‹ˆë‹¤'], ['êº…'], ['ê¿ê¿í•˜ê²Œ'], ['ë§Œë“œì…§ì–´'], ['ì‚¬ë‘ì´ë¤„ì§€ê¸¸ë°”ëŸ¤ëŠ”ë°'], ['ê´¸ì°®ë‹¤'], ['ë´£ë‹¤'], ['ë§¬'], ['ë¯•ì§€ê°€'], ['í› ì–´ì–¼ì”¬'], ['ì›©'], ['êµ¬ê°€ì˜ì„œê°€í›¨ì”¬ë‚«ë‹¤ã…‰'], ['ë‘ë²ˆë´£ëŠ”ë°'], ['í€µì„'], ['ë²ˆì¨°ì—ì„œ'], ['í‹°ë¹„ë¡œë´£ëŠ”ë°ì¼ì„œìš”'], ['ì–´ë”¨ìœ¼ë´'], ['ê´ì°®ì•˜ë‹¤'], ['ì•³í‚¨ìŠ¤'], ['ìˆ˜ê³ í•˜ì…§ìŠµë‹ˆë‹¤'], ['ã…‘ã…‹', 'ã…‘', 'ã…‘'], ['ë½ˆë…¸'], ['ì˜í™”ë„¤ìš¥'], ['ì˜í˜¸ã…“'], ['ë°˜ë‹´íš½'], ['ê²œ'], ['í¸ì§‘ë¬ë‹¤ëŠ”ê²Œ', 'ë¹‚ì¸ ê°”ë”ë‹ˆ', 'ë™‡'], ['ë—€ì ì´'], ['ì§‘ì¤‘ì˜ì•ˆëë˜'], ['ë©‹ì´ì‘ˆì–´ì—¬'], ['í­ê·„ë§¨'], ['ì•œã…‹ã…‹ì›ƒëŠ”ê±°ì•¼', 'ìš°ëŠ”ê±°ì–”ã…‹ã…‹ã…‹'], ['ë§¤ì ¬ê²Œ'], ['ì‚¬ëŒì´ì €ë ‡ê²Œí™±í™±ë³€í• ìˆ˜ê°€ìˆë‚˜ì–´ì²˜êµ¬ë‹ˆì—†ë‹¤'], ['í—¿'], ['í›ê±¸ì—ì˜í•œ', 'í›ê±¸ì„ìœ„í•œ', 'í›ê±¸ì—'], ['ã…‰ã…‰'], ['ã…¤ã……ã…ã……'], ['ê²€ìƒ‰í•´ë´¤ëŠ”ëã…‹ã…‹'], ['í™§íŒ…'], ['ìˆ€'], ['ì†Œê¿‰ë†€ì´í•˜ëŠ”'], ['ë˜ê³ ì‹¶ì–´ìš¯ã…ã…'], ['ìƒ¹ã…‚ë‚˜ã…“'], ['í‹€ì–´ì£¼ëŠ”ê±°ë´£ëŠ”ëŒ€'], ['ì™¤ì¼€'], ['ë§ˆì§€ë§‰íšì¤„ëª¨ë¥´ê³ ë´£ëŠ”ë°'], ['ì‘ˆë¥¼', 'ì‘ˆ', 'ì‘ˆ', 'ì‘ˆ'], ['ã…‰ã…‰ë³¼ì¤„ëª¨ë¥´ë©´'], ['ã…‰ã…‰'], ['ë§ì£µ'], ['ì°ì°í•˜ê³ '], ['ì™¤ì¼€'], ['ì¢ ëŠ”ë°'], ['ì•¡ìˆ€'], ['ë°˜ê°ë¬ë‹¤'], ['ê¼¬ê¼¬ë§ˆë“¤ã…‰ã…‰'], ['ã…‰ã…‰ã„±'], ['ì–'], ['ì™„ì „ì™„ì¤œ'], ['ë¬ëƒ'], ['ì¡°ë‹ˆë'], ['ë¡œì™„ì•³í‚¨ìŠ¨'], ['í™”ì”¨ê³¼í•¨ê¼'], ['ë´£ëŠ”ë°'], ['ë¿…'], ['í˜¸í˜¸í™'], ['ì™¤ìº'], ['ã…‰ã…‰'], ['ì›¤ì„'], ['ë¬´ì¡°ê»€'], ['ì œã…”ã…”ë°œ'], ['ì—­ì‹œì¼ë³¸ê²ƒì¹»ë”ë‹ˆì¼ë³¸ê±°ë‚´'], ['ì–´ë¦´ë–„'], ['ì¼ìˆì—ˆëŠ£ë°'], ['í€µ'], ['ë¹•ë‹ˆë‹¤'], ['ë–„ëŠ”'], ['ì´ë‹¤'], ['ã…“ã…‹ã…‹ã…‹'], ['ê´œíƒ†ì€'], ['ê´¸ì°¬ìœ¼ë‚˜'], ['ëª…ê»€'], ['ì¼ê²Œë´¤ëŠ”ë°í‰ì •ì´ì˜¤ã…ì´ëŸ¬ì§€ã…‹ã…‹'], ['ì•'], ['ë¹‚ì´ì–´ì„œ'], ['ì´ì˜í™”ì§„ã„´ì§œã„±ã…ê°™ìŒ'], ['ê²œë¸”ëŸ¬'], ['ë™‡'], ['ì›ƒì—ˆë„¼ã…‹'], ['ì¬ë°ŒìŠµë‹ˆë‹¤í•˜í•˜í•³íˆí£'], ['ì†“ëŠ”'], ['ì˜ì°ì°í•˜ê³ '], ['íˆíˆí›íˆí›íˆíˆíˆí›í›'], ['ë´£ìŠµë‹ˆë‹¤'], ['ì‹œí‚µì‹œë‹¤'], ['ìµ¯í•œì˜'], ['ë¬´ê¸°ì§€ë©±'], ['ì™¸ìƒìˆí„°ì˜ˆì‚°ì¢€ëŠ˜ë ¤ì£¼ì„¸ìš”'], ['ë­ê²Œ'], ['ì£¼ì…§ìœ¼ë©´í•©ë‹ˆë‹¤'], ['ëŒ€íŠ±ë ¹ê°í•˜'], ['ì‘ˆ'], ['ì³¬ê³„ì ìœ¼ë¡œ'], ['ì•„í”•ë‹ˆë‹¤'], ['ëƒê¸€'], ['ì•ˆì§€í‚µë‹ˆë‹¤'], ['ëª…ì˜ˆí™°ì†'], ['í–ì£¼ì„¸ìš”'], ['ë“±êµ£ê¸¸'], ['í•œêµ­ì„ì§€í‚µì‹œë‹¤'], ['ë¹•ë‹ˆë‹¤'], ['êµ­ë°ˆí˜ˆì„¸'], ['ì •ì¹˜ì‘ˆ'], ['ì± ëŸ‰'], ['ì¼ìœ¼í‚µì‹œë‹¤'], ['ê´´ì”¸í•©ë‹ˆë‹¤'], ['ì„¸ì›ì‹œë‹¤'], ['ì‹œí‚µì‹œë‹¤'], ['í‘€í–‰'], ['íŒŒë©´í•´ì•¼ëë‹ˆë‹¤'], ['í½ë‹ˆë‹¤'], ['ë¹„ì ¼ì„'], ['ê´¸í•˜ì—¬'], ['ë§ˆì„¸ìš”á†¢á†¢'], ['ì± ëŸ‰'], ['í¬í•¨ì‹œí‚µì‹œë‹¤'], ['ë”¸ê´¸'], ['ëœ¹ë‹ˆë‹¤'], ['ëµŒ'], ['ë„µ'], ['íˆì´ì•¼'], ['ë­”ë°ìš¬ã…‹ã…‹'], ['ì§„ã…‰'], ['ì­ˆë¼›ì­ˆë¼›'], ['ëµ'], ['ì†¨ì¥ë‹˜ì€', 'ì•„ë””ë ìŠ¤'], ['ì‹ìŠµê´€ì…ë‹ˆë ì•„'], ['ë¬ì–´ìš”'], ['ë„¤ë„µ'], ['ë³´ìƒ¸ë‚˜ìš”'], ['ê·€ì—¼ë½€ì¨•í•´ë³´ì´ë„¤ìš”'], ['ììœ±ë‹ˆë‹¤'], ['êº„ì•„'], ['ìµì˜¤ë„¤ìš”'], ['ë´¬ìš”'], ['ë„µ'], ['ì¦'], ['ëµŒë‹¤ê³ '], ['ëŠë‚ë‹ˆë‹¤'], ['ë§Œë‚˜ëµ€ìŠµë‹ˆë‹¤'], ['ë„¤ë„¤ë„µ'], ['ì•–ë„¤ì—¬'], ['ë¯“ì°Œê²Œ'], ['ì»¤í”¼ì±—'], ['êº„ì•„'], ['ë„¤ë„µ'], ['ì§„ì´¤', 'ì•„ëŠ¼ë‹ˆê½ˆ'], ['ë„µë„µ'], ['ìŠ¬íìŠµë‹ˆë‹¤'], ['ëµí…ë°'], ['ë„µ'], ['ë””ìŸŒíŒ€'], ['í‚í‚í‚'], ['ë„µë„µ'], ['ê»ë‹¤'], ['ëµê²Œìš”'], ['ã…‹ã…‹ã…‹ì ë‹¹í•´ë³´ì—¬ìš¥'], ['ì´ë¿¨ìš”ì˜¤'], ['ì„œê³„ì‹œì§˜ã…‹'], ['ì˜™'], ['í‚'], ['ì™•ê²œ'], ['ë´¬ìš©'], ['í½ë‹ˆë‹¤'], ['ë´¬ì„œ'], ['ã…‹ã…‹ì•œã…‹ã…‹ã…‹ì°¸ìƒˆ'], ['í–'], ['ì†Œë“•í•´ë³´ì—¬ìš”'], ['ë„µ'], ['ë„µ'], ['êº„í•«'], ['ëµŒê¹€ì—'], ['ë„µ'], ['í•¨ê¼í•˜ê¸°ë¡œ'], ['ë´¬ìš”'], ['ì¢‹ìŠµë‹ˆë‹¼ã…‹ã…‹'], ['ë„µ'], ['ì±—ë´‡ì—'], ['ëµŒ'], ['ëµŒ'], ['ì‹œí‚µì‹œë‹¤'], ['ê°€ê²Œë§ˆì”¸'], ['ìŠ¤ì¼€ì¥´ì´'], ['ì—­ì‹€'], ['ë½œíŒ…í•˜ì„¸ìš”'], ['ì™˜ã…‹ã…‹'], ['ë„µ'], ['ëµ'], ['ì±•í„°'], ['ë´¬ìš”'], ['ëŒ€ì£¤ë§›ì…ë‹ˆë‹¤'], ['ëµ'], ['ë°”ëŒì¬ê³ '], ['ì•„í”•ë‹ˆë‹¤'], ['ì—Šê·¸ì €ê»˜'], ['ì„¤ê²†ì´'], ['ìœ íŠ­ì—'], ['ìˆëŠ”ê±´ê°€ìš¬ã…‹'], ['ì»¤í”¼ì±—ìœ¼ë¡œ'], ['ìš°ì™˜ã…‹ã…‹ã…‹ì €'], ['ê³ ê³ í•´ë´…ì‹œë‹¼ã…‹ã…‹'], ['ìŠ¤ì¾ƒ'], ['ëµ'], ['ë´¬ìš”'], ['ë´¬ìš”'], ['ìœ íŠ­ë¼ë„'], ['ì¦¤ì§‘'], ['ë¼›ì†ê¹Œì§€'], ['ì•œã…‹ã…‹', 'í™“ã…‹ã…‹ã…‹'], ['ë©¥'], ['ì•œã…‹ã…‹'], ['ì‹¬ë©'], ['ë„µ'], ['ë˜ë´¬ìš”'], ['ë‹‰ë„´ì€'], ['êº„ì•„'], ['êº„ì•„'], ['ìˆì„ë–„'], ['í‹°ì±—'], ['ì›ƒê²¨ìš¬ã…‹ã…‹'], ['ì¦ê±°ì› ì–´ì˜„ã…‹ã…‹'], ['ì¬ëŸ¬ê°€ìš”'], ['ë¯±'], ['ìŠ¤ì›»ì…”ì¸ ë¥¼'], ['ì˜ˆì©ë‹ˆë‹¤'], ['ì¬ê³ '], ['ì‹¶ì–´ìˆ´'], ['ë‚‘ë‚‘ëŒ€ëŠ”ê±°'], ['ì§„ì‹¬ì´êµ°ìš¬ã…‹ã…‹'], ['ë´¬ìš”'], ['ë°”ë€ë‹ˆë‹¤'], ['ìŠ¤íƒ€ë€ì˜'], ['ìŠ¤íƒ€ë€ì´ë‚˜'], ['ë°”ë¥´ë€ì˜'], ['í­ê·„ì´'], ['ì™„ì ¼'], ['ì˜¤ì§€ë–ì—'], ['ì•„í”•ë‹ˆë‹¤'], ['ì†Œì†Œí•œì¬ë¯¸ã…œã…‹ì—¬íƒ¯ê»ì‚¶ì„ë‹¤ì‹œëŒì•„ë³´ê²Œí•˜ëŠ”ì˜í™”ã…œ'], ['ì§¦ë‹¤ëŠ”ê²Œ'], ['ï½€'], ['ì´'], ['ë²„ì ¼ìœ¼ë¡œ'], ['ì©'], ['ìµì˜¤ë„¤ìš”'], ['ê»ë‹¤'], ['ë¬´ì„œì›Ÿì–´'], ['ëµ›ì§€ë§Œ'], ['ìˆ˜ê³ í•˜ì…§ìŠµë‹ˆë‹¤'], ['ë€°ì¼ì´ë‹¤ë€°ì¼ì´ë¼êµ¬ì—¬'], ['ã…Šìµœê³ ì˜'], ['ë¯¸ì‰˜ëˆ„ë‹˜ì˜'], ['ì¯§ì¯§'], ['ë¯¸ì‰˜íŒŒì´í¼'], ['ì•„ê¹Œì›Ÿë‹¤ê³ '], ['ìš¸ë»”í–‡ë‹¤í'], ['ì•ˆë‚˜ì˜¨ë‹¼ã…‹ã…‹'], ['ì¬ë°‹ê²Œë´£ì–´ìš©'], ['í›ê±¸ì—'], ['ë³´ì…©'], ['íë¯“í•˜ë„¤ìš”'], ['ìŠ¬íë‹¤'], ['ì¬ë¯¸ì‡ã„±ã…”ë³¸ì˜í™”ì—ìš©ã…'], ['ë¬ëŠ”ë°'], ['ì•„ì‰¬ì›Ÿì§€ë§Œ'], ['ì´ê²Œì˜í™¥ë‹ˆê¹Œ'], ['ì‹œê°„ê°€ëŠ”ì¤„ëª¨ë¥´ê³ ë´£ë„¤ìš”'], ['ì™¤ì¼€ì´ì¨'], ['ê´¸ì°®ì€ë°'], ['ê»ë‹¤'], ['ì°ì°í•˜ê³ '], ['ëˆ™ë¬¼'], ['í˜“ì†Œë¦¬'], ['ë­ì–´ë‚˜ëŠ”'], ['ì¬ë°Œê²Œë´£ë˜ì˜í™”'], ['ì „ê°œë¬ë‹¤'], ['í'], ['ã…‰ã…‰ã…‰'], ['ì°ì°í•œ'], ['ì¬ë¯¸ì—†ë‹¤ì¬ë¯¸ì—†ì–´ã…‰ã…‰'], ['í‰ì ë‚¯ì¶”ê¸°ì„±ê³µí•¨ã……ã„³ã„±'], ['ë¡œë³´ìº…'], ['ì–´ë ¸ì„ë–„ë³´ê³ '], ['ë«ì„ê¹Œ'], ['ì˜¤ë¹ ë“¤ë”ë©‹ì‡ì–´ì¡‹ì–´ìš”ã…ã…'], ['ë¯¸ìŠ¤í…ŒìŒ'], ['ì³‡ë°”í€´ì†ì—ì„œ'], ['í—ì¶‹ë¥˜í›ƒ', 'ã…'], ['ìµì˜¤'], ['ì¼ì ìš”'], ['ë¹•ë‹ˆë‹¤'], ['ê²°ë§ì€ì°ì°í•˜ë‹¤'], ['ë´¤ëŠ¡ë°'], ['ì˜í™”ë°¨ë„¤ìš”'], ['ë²´ì–´ìš”'], ['ë´£ëŠ”ë°'], ['ì©'], ['ìŠ¤íƒ€ë€'], ['í˜•ì‚¬ë­'], ['ì•œã…‹ã…‹'], ['ë•ˆ'], ['í‰·'], ['ì´ì˜ë„ã…œã…œ'], ['ê¸°ëŒ€í•˜ê³ ë´£ë”ë‹ˆ'], ['ì™¤ì¼€'], ['ìŠ¤íƒ€ë€'], ['ë¿œì—‡ë‹¼ã…‹'], ['ì•„í•˜í•˜í•˜í•³í•³í•˜í•˜í•³í•³í•³'], ['ì¬ë¯¸ì‡ì£µ'], ['ë´£ë„¤'], ['â˜¼ë¬¸ì¬ì¸ì •ë¶€ëŠ”'], ['ì§€í‚µì‹œë‹¤'], ['ì†Œë…„ë²•íì§€í•´ì£¼ã…›ã…”ìš”'], ['ìœ„í–'], ['ì–´ì©ë‹ˆê¹Œ'], ['ì‹œí‚µì‹œë‹¤'], ['ë‚´ë ¤ì£¼ì…§ìœ¼ë©´'], ['í¬í•¨ì‹œí‚µì‹œë‹¤'], ['ë‚Ÿë‚Ÿíˆ'], ['ğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘Œ'], ['ë„µ'], ['ì„¤ë œë‹¤'], ['ì¬ê³ '], ['ë¹•ë‹ˆë‹¤'], ['ëŠë‚ë‹ˆë‹¤'], ['ë„µ'], ['ë¹•ë‹ˆë‹¤'], ['ë„µ'], ['ë„µ', 'ë„µ'], ['ê·œìˆ‘'], ['ê¸°ì©ë‹ˆë‹¤'], ['ê»ë‹¤ê°€'], ['ì¼¤ë ˆë¥¼'], ['ê¸°ì©ë‹ˆë‹¤'], ['17ì¼¤ë ˆë§Œ'], ['ì™•ì ¬'], ['êº„ì˜¤ì˜¤ì˜¬'], ['ëµ'], ['ê°¬ì„±ì—'], ['ë§Œë‚˜ëµŒ'], ['ìš°ìŒ°ìš°ìŒ°'], ['ã…‹ã…‹ë„µ'], ['êº„', 'í›…'], ['í‹€ì–´ë†“ë‚˜ìš¬ã…‹ã…‹'], ['ì‚¬ì§„ì…ë‹ˆê½ˆ'], ['ë„µ'], ['ë„µ'], ['ê°ìƒ¤í•©ë‹ˆëŒ±'], ['ë†‹ë¶'], ['ë¹•ë‹ˆë‹¤'], ['ë„µ'], ['ì‰‘ì‰‘ë²„ê±°'], ['í•˜ì‹œì£µ'], ['ì—Šê·¸ì œ'], ['í› ê¶ˆ'], ['ê³ ì–‘ì´ëœã…‹ã…‹'], ['ì–´ë”¨ë”ë¼'], ['í›„ì›í•˜ê²Œ'], ['ëµŒê²ƒ'], ['ëµê²Œì—¬'], ['ìš”ë¡·ê²Œ', 'ìš”ë¡·ê²Œ'], ['ì–´êº ì¶¤ì„'], ['ë„µ'], ['ë„µ'], ['í™§íŒ…í•˜ì„¸ìš”'], ['ì•œã…‹ã…‹'], ['ì¼í•©ì‹€ë‹¤ì•„ì•„'], ['í£'], ['ì¦ê±°ì›ŸìŠµë‹ˆë‹¤'], ['í›…íŒ€ì˜'], ['êº„ì•„ì•„ì•„'], ['ëšœë ·í•´ã…›ã…“'], ['ë§Œë‚˜ëµ'], ['ëº„'], ['ë´¬ìš”'], ['ë„µ'], ['ì«‘ë—í•˜ë„¤ìš”', 'ì˜í©'], ['ëµ'], ['êº„ì•„'], ['ë„µì€'], ['ë„¤ë„µ'], ['ã…‹ã…‹ìœ íŠ­ì—'], ['ëµ'], ['ê¹ƒí—™ì—'], ['ì»¤í”¼ì±—ë†“ì³¤ë„¤ìš”'], ['ë„µ'], ['ë´¬ìš”'], ['ëµ', 'ë´¬ìš”'], ['í•´ë´…ì‹œë‹¼ã…‹ã…‹'], ['ë°€í‘€ìœ ë‚˜ë² '], ['ìœ íŠ­ì¤‘'], ['ì±•í„°ë¥¼'], ['í£'], ['ê°€ëŠ”êµ°ë‡½'], ['êµ '], ['ì§„ì± ', 'ì‹œì‘í•¨ë '], ['ëµ'], ['ëµìˆ˜'], ['ëµê²Œìš©'], ['ë„µë„µ'], ['ì•„ë‹ ã…‹ã…‹', 'ëŒ€ì¥ì´ì—ì˜„ã…‹ã…‹ã…‹'], ['ë¡œìš°ë¼ì´ì¦ ã…‹ã…‹ã…‹', 'ë¬¸ì œë„¤ì˜„ã…‹ã…‹'], ['ì¢ì¢'], ['ë“„ì„', 'ë“„'], ['ë„µ'], ['ë„µë„µ'], ['ë„ˆë¬´ê·€ì—½ë‹¼ã…‹ã…‹íã… '], ['ë–„ë ¤ì£¼ì„¸ìš”'], ['ë„µ'], ['í•³'], ['ë´¬ìš”'], ['ê·€ì—½êµ°ìš¬ã…‹ã…‹'], ['í€µ'], ['ì½§ë°”ëŒì¬ëŸ¬'], ['ì•œã…‹ã…‹ì•„ì‰½ê²Œë„'], ['ã…ˆã…”ê°€', 'ìœ¼ì± '], ['ëµŒ'], ['êµ°ë‡½'], ['ì¬ê³ '], ['ë§Œë‚˜ëµê²Œìš”'], ['í€µ'], ['ê°­ì°¨ì´'], ['ì¬ë°Œì—ˆìŠµë‹ˆë‹¼ã…‹ã…‹'], ['ì‹œì‘í•´ë³´ì‹œì£µ'], ['ë„µ'], ['ì•œã…‹ã…‹í¬ì¦ˆ', 'ì €ê±°ì˜€êµ°ìš¬ã…‹ã…‹']]\n",
      "unkë¡œ ì¸ì‹ëœ í† í°ì˜ ì´ ê°œìˆ˜ëŠ” 478 ê°œ ì…ë‹ˆë‹¤.\n",
      "unk ë¡œ ì¸ì‹ëœ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„: unk_tokens_train.csv\n"
     ]
    }
   ],
   "source": [
    "# train dataì—ì„œ ì–´ë–¤ ë‹¨ì–´ê°€ <UNK>ë¡œ ì¸ì‹ë˜ëŠ” ê²ƒì¸ì§€ í™•ì¸í•´ë³´ì.\n",
    "\n",
    "# ì¬ë°ŒëŠ” ê²ƒ: encode = tokenizer(text) ì˜ ê²°ê³¼ëŠ” transformerì— ì†í•˜ëŠ” íƒ€ì…ìœ¼ë¡œ, ë‚´ì¥ í•¨ìˆ˜ê°€ ëª‡ ê°€ì§€ ìˆë‹¤. ex. token_to_char ...\n",
    "#print(type(encoded))\n",
    "\n",
    "# 0. unk_pdê°€ ì €ì¥ë˜ì—ˆë‹¤ê³  ê°€ì •.\n",
    "\n",
    "# 0-1. setting\n",
    "import copy\n",
    "# 1. ë¬¸ì¥ì— ëŒ€í•œ ë°˜ë³µ\n",
    "\n",
    "def find_unk_tokens(ex_unk_sentences):\n",
    "    unk_tokens = []\n",
    "    count = 0\n",
    "    \n",
    "    for unk_sentence in ex_unk_sentences:\n",
    "        _unk = []\n",
    "        # 2. encode and decode\n",
    "        encoded = tokenizer(unk_sentence)                                \n",
    "        decoded = tokenizer.convert_ids_to_tokens(encoded['input_ids'])     # ê° token idë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¦¬í„´\n",
    "\n",
    "        # 3. ëª‡ ë²ˆì§¸ê°€ unkì¸ì§€ í™•ì¸. ì¸ë±ìŠ¤ ëª¨ë‘ ì €ì¥\n",
    "        unk_indexes = [i for i, token in enumerate(decoded) if token == tokenizer.unk_token]\n",
    "        count += len(unk_indexes)\n",
    "\n",
    "        # 4. inputì—ì„œ ì–´ë–¤ ë‹¨ì–´ê°€ unkì˜€ëŠ”ì§€ í™•ì¸\n",
    "        for index in unk_indexes:\n",
    "            char_index = encoded.token_to_chars(index)\n",
    "            original_token = unk_sentence[char_index.start:char_index.end]  # char_index ëŠ” CharSpan(start=15, end=19) í˜•íƒœë¡œ ë¦¬í„´ë˜ë”ëë‹ˆë‹¤... ì‹ ê¸°!\n",
    "            \n",
    "            #print(f\"<UNK> token: {original_token}\")\n",
    "            _unk.append(original_token)\n",
    "        \n",
    "        if _unk:\n",
    "            unk_tokens.append(_unk)\n",
    "            \n",
    "    return unk_tokens, count\n",
    "\n",
    "unk1_sentences, s1_count = find_unk_tokens(unk_pd['unk1_sentences'].values.tolist())\n",
    "unk2_sentences, s2_count  = find_unk_tokens(unk_pd['unk2_sentences'].values.tolist())\n",
    "unk_tokens = copy.deepcopy(unk1_sentences)\n",
    "unk_tokens.extend(unk2_sentences)\n",
    "print(f\"unk ë¡œ ì¸ì‹ëœ í† í°ë“¤ : {unk_tokens}\\nunkë¡œ ì¸ì‹ëœ í† í°ì˜ ì´ ê°œìˆ˜ëŠ” {s1_count + s2_count} ê°œ ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "# 5. ê¸°ì¡´ unk_pd(sentenceê°€ ì €ì¥ëœ csv) ì— ì €ì¥\n",
    "# 5-1. ê¸¸ì´ ë§ì¶”ê¸°\n",
    "max_len = max(len(unk1_sentences), len(unk1_sentences))\n",
    "unk2_sentences += [''] * (max_len - len(unk2_sentences))\n",
    "unk_pd['unk1_token'] = unk1_sentences\n",
    "unk_pd['unk2_token'] = unk2_sentences\n",
    "\n",
    "unk_pd = unk_pd[['unk1_sentences', 'unk1_count', 'unk1_token', 'unk2_sentences', 'unk2_count', 'unk2_token']]\n",
    "unk_pd.head(5)\n",
    "\n",
    "# # 5. csv ë¡œ ì €ì¥\n",
    "# unk_tokens_pd = pd.DataFrame({'unk_token': unk_tokens})\n",
    "# unk_tokens_pd.to_csv(\"./unk_tokens_train.csv\")\n",
    "unk_pd.to_csv(\"./unk_tokens_train.csv\")\n",
    "\n",
    "print(f\"unk ë¡œ ì¸ì‹ëœ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„: unk_tokens_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. UNKë¥¼ í¬í•¨í•œ ë¬¸ì¥ë“¤ì„ spelling check ì§„í–‰\n",
    "- git clone í•˜ê³ ,\n",
    "- pyhanspell -> issue ë“¤ì–´ê°€ì„œ ë°”ê¾¸ë¼ëŠ”ëŒ€ë¡œ ë°”ê¾¸ê³  ì¬ì„¤ì¹˜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "'py-hanspell'ì— ë³µì œí•©ë‹ˆë‹¤...\n",
      "remote: Enumerating objects: 101, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 101 (delta 5), reused 10 (delta 3), pack-reused 81\u001b[K\n",
      "ì˜¤ë¸Œì íŠ¸ë¥¼ ë°›ëŠ” ì¤‘: 100% (101/101), 25.27 KiB | 8.42 MiB/s, ì™„ë£Œ.\n",
      "ë¸íƒ€ë¥¼ ì•Œì•„ë‚´ëŠ” ì¤‘: 100% (42/42), ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ssut/py-hanspell.git\n",
    "\n",
    "# ì´í›„ https://github.com/ssut/py-hanspell/issues/31 ì— ë”°ë¼ íŒŒì¼ ë‚´ìš© ìˆ˜ì • í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ilewis/git_dir/level1_semantictextsimilarity-nlp-14/JH/data_analysis/py-hanspell\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  warnings.warn(\n",
      "running install\n",
      "/Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing py_hanspell.egg-info/PKG-INFO\n",
      "writing dependency_links to py_hanspell.egg-info/dependency_links.txt\n",
      "writing requirements to py_hanspell.egg-info/requires.txt\n",
      "writing top-level names to py_hanspell.egg-info/top_level.txt\n",
      "reading manifest file 'py_hanspell.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'py_hanspell.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.macosx-11.1-arm64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.macosx-11.1-arm64/egg\n",
      "creating build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/constants.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/__init__.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/response.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/spell_checker.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/constants.py to constants.cpython-39.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/response.py to response.cpython-39.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/spell_checker.py to spell_checker.cpython-39.pyc\n",
      "creating build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/PKG-INFO -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/SOURCES.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/dependency_links.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/requires.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/top_level.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/py_hanspell-1.1-py3.9.egg' and adding 'build/bdist.macosx-11.1-arm64/egg' to it\n",
      "removing 'build/bdist.macosx-11.1-arm64/egg' (and everything under it)\n",
      "Processing py_hanspell-1.1-py3.9.egg\n",
      "Removing /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/py_hanspell-1.1-py3.9.egg\n",
      "Copying py_hanspell-1.1-py3.9.egg to /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "py-hanspell 1.1 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/py_hanspell-1.1-py3.9.egg\n",
      "Processing dependencies for py-hanspell==1.1\n",
      "Searching for requests==2.28.1\n",
      "Best match: requests 2.28.1\n",
      "Adding requests 2.28.1 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for certifi==2022.9.24\n",
      "Best match: certifi 2022.9.24\n",
      "Adding certifi 2022.9.24 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for urllib3==1.26.11\n",
      "Best match: urllib3 1.26.11\n",
      "Adding urllib3 1.26.11 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for idna==3.3\n",
      "Best match: idna 3.3\n",
      "Adding idna 3.3 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for charset-normalizer==2.0.4\n",
      "Best match: charset-normalizer 2.0.4\n",
      "Adding charset-normalizer 2.0.4 to easy-install.pth file\n",
      "Installing normalizer script to /Users/ilewis/opt/anaconda3/bin\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Finished processing dependencies for py-hanspell==1.1\n",
      "/Users/ilewis/git_dir/level1_semantictextsimilarity-nlp-14/JH/data_analysis\n"
     ]
    }
   ],
   "source": [
    "%cd ./py-hanspell\n",
    "!python ./setup.py install\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§ì¶¤ë²• êµì • í›„ ë³µì›í•œ ë¬¸ì¥ì„ ì˜†ì— ë¶™ì¼ ì˜ˆì •ì…ë‹ˆë‹¤. ìœ„ ì…€ë“¤ë¡œë¶€í„° ì´ì–´ì„œ, unk_pd ê°€ ë¶ˆëŸ¬ì™€ì ¸ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "# 0. settings\n",
    "import sys\n",
    "sys.path.append('./py-hanspell')\n",
    "from tqdm import tqdm\n",
    "from hanspell import spell_checker\n",
    "\n",
    "# 1. ìŠ¤í ë§ ì²´í¬ë¥¼ ì§„í–‰í•  ë¬¸ì¥ ì„¸íŠ¸ ì¤€ë¹„\n",
    "u1_s = unk_pd['unk1_sentences'].values.tolist()\n",
    "u2_s = unk_pd['unk2_sentences'].values.tolist()\n",
    "u2_s = [item for item in u2_s if item]\n",
    "\n",
    "# 2. í•¨ìˆ˜ ì„ ì–¸\n",
    "def check(data):\n",
    "    changed, label = [], []\n",
    "    \n",
    "    for sentence in tqdm(data):\n",
    "        # 2-1. ìŠ¤í ë§ ì²´í¬\n",
    "        result = spell_checker.check(sentence).as_dict()\n",
    "        checked = result['checked']\n",
    "        \n",
    "        # 2-2. ê²°ê³¼. ë¬¸ì¥ì´ ì „~í˜€ ë°”ë€Œì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ unchangedë¼ê³  ê¸°ë¡ë˜ê²Œ í•¨. ë‹¨ìˆœíˆ ë„ì–´ì“°ê¸°ë§Œ ë“¤ì–´ê°€ë„ changed ë¡œ ì¸ì‹ë¨.\n",
    "        changed.append(checked)\n",
    "        label += [\"changed\" if sentence!=checked else \"unchanged\"]\n",
    "\n",
    "    return changed, label\n",
    "\n",
    "# 3. pd ì €ì¥\n",
    "changed_1, label_1 = check(u1_s)\n",
    "changed_2, label_2 = check(u2_s)\n",
    "\n",
    "# 3-1. ê¸¸ì´ê°€ ì„œë¡œ ë‹¤ë¥¸ ë¦¬ìŠ¤íŠ¸ë¥¼ 'ì—´' ë¡œ ë¶™ì—¬ë„£ìœ¼ë ¤ê³  í•˜ë‹ˆê¹Œ, ê¸¸ì´ê°€ ì•ˆ ë§ìœ¼ë©´ ì•ˆ ë˜ì„œ, ì„ì‹œ ë°©í¸ìœ¼ë¡œ ë„£ì–´ë’€ìŠµë‹ˆë‹¤. ì´ê±°ë•Œë¬¸ì— ì¢€ ì˜¤íˆë ¤ ë³µì¡í•´ì ¸ì„œ ìˆ˜ì •ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "changed_2 += [''] * (len(changed_1) - len(changed_2))\n",
    "label_2 += [''] * (len(label_1) - len(label_2))\n",
    "\n",
    "unk_pd['unk1_sentences_checked'] = changed_1\n",
    "unk_pd['unk1_checked_label'] = label_1\n",
    "\n",
    "\n",
    "unk_pd['unk2_sentences_checked'] = changed_2\n",
    "unk_pd['unk2_checked_label'] = label_2\n",
    "\n",
    "unk_pd = unk_pd[['unk1_sentences', 'unk1_count', 'unk1_token', 'unk1_sentences_checked', 'unk1_checked_label', \n",
    "                 'unk2_sentences', 'unk2_count', 'unk2_token', 'unk2_sentences_checked', 'unk2_checked_label']]\n",
    "\n",
    "unk_pd.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
